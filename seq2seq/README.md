> *seq2seq* models.

* Notes:

    - Neural Machine Translation (NMT) one of the first testbeds of *seq2seq* models.

    - NMT mimics the behaviour of understanding (encoding) a given sentence and then translating (decoding). 
        In another word a **Encoder-Decoder** architecture.

    - Recurrent Neural Network (RNN) is a natural choice for sequential data, used by most NMT models. However it 
        could be uni or bidirectional, single or multi-layer, vanilla or a Long Short-term Memory (LSTM) RNN.

---
* Resources:
    - [Tensorflow NMT](https://github.com/tensorflow/nmt)
    - [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

